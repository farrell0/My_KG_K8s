#
#
#  This (values_override.yaml) file contains environment variable
#  references that need to be substituted before use.
#
#  More,
#
#     .  The environment variables, and their associated values are
#        kept in file, 20*
#     .  The program to do this substitution is file, 45*
#        File 45 will output a version of this file where the environment
#        variables are replaced with the proper/actual values.
#
#     .  Good reference; see,
#           https://github.com/KatanaGraph/argocd-apps-product/tree/main/continuous/gke
#           https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/
#


global:
  katana:
    image:
      dask:
        scheduler: gcr.io/${GCP_PROJECT_NAME}/katana-notebook
        worker: gcr.io/${GCP_PROJECT_NAME}/katana-notebook

      katana:
        controller: gcr.io/${GCP_PROJECT_NAME}/katana-notebook
        launcher: gcr.io/${GCP_PROJECT_NAME}/katana-notebook
        notebook: gcr.io/${GCP_PROJECT_NAME}/katana-notebook
        worker: gcr.io/${GCP_PROJECT_NAME}/katana-notebook

      tag: ${KATANA_VERSION_FIXED}

    #  This will set placement of the KG workers; nodepool wise
    #
    workloadPlacement:
      nodeSelector:
        node.kubernetes.io/instance-type: ${KATANA_WORKER_VM_TYPE}
        topology.kubernetes.io/zone: ${GCP_ZONE}

      podAntiAffinityPreset: "hard"

      tolerations:
        # Tolerate the no schedule placed on the graph worker nodes
        #
        - effect: "NoSchedule"
          key: "node-role.kubernetes.io/katana-graph-worker"
          operator: "Equal"
          value: "true"


#####################################
#####################################


etcd:
  nodeSelector:
    node.kubernetes.io/instance-type: ${KATANA_CONTROL_VM_TYPE}

  #  Workload spread; this might be working
  #
  spec:
    topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway

  #  This is new, from DeepCDR
  #
  resources:
    limits:
      cpu: 500m
      memory: 256Mi
    requests:
      cpu: 500m
      memory: 256Mi
  
  tolerations:
    - effect: "NoSchedule"
      key: "node-role.kubernetes.io/katana-control-plane"
      operator: "Equal"
      value: "true"


#####################################
#####################################


jupyterhub:

  proxy:
    chp:
      nodeSelector:
        node.kubernetes.io/instance-type: ${KATANA_CONTROL_VM_TYPE}
      resources:
        limits:
          cpu: 500m
          memory: 256Mi
        requests:
          cpu: 500m
          memory: 256Mi
    service:
      type: LoadBalancer


  hub:
    nodeSelector:
      node.kubernetes.io/instance-type: ${KATANA_CONTROL_VM_TYPE}
    config:
      Authenticator:
        admin_users:
          - kadmin1
        allowed_users:
          - ${MY_USERNAME}
      DummyAuthenticator:
        password: ${MY_PASSWORD}
      JupyterHub:
        authenticator_class: dummy
    resources:
      limits:
        cpu: 500m
        memory: 1Gi
      requests:
        cpu: 500m
        memory: 1Gi


  #  While called 'singleuser', this is for the Notebook proper
  #
  singleuser:
    serviceAccountName: ${KATANA_SERVICE_ACCOUNT_NAME}
    cloudMetadata:
      blockWithIptables: false
    #
    #  Locates; notebook
    #
    nodeSelector:
      node.kubernetes.io/instance-type: ${KATANA_WORKER_VM_TYPE}
    cpu:
      limit: 1
      guarantee: 1
    memory:
      limit: 8G
      guarantee: 8G


#####################################
#####################################


katana:

  config:
    # dev:
    #   kube:
    #     max_workers: 10
    logging:
      min_log_level: debug
    object_store:
      bucket: ${GCS_BUCKET}
    dev:
      kube:
        max_workers: 20
      scheduler:
        idle_timeout: 1h

  serviceAccount:
    create: false
    name: ${KATANA_SERVICE_ACCOUNT_NAME}


  #  Dask administrative stuff-
  #
  #  dask-scheduler
  #
  dask:
    cluster:
      spec:
        scheduler:
          spec:
            nodeSelector:
              node.kubernetes.io/instance-type: ${KATANA_CONTROL_VM_TYPE}
            #
            #  This is new, from DeepCDR
            #
            tolerations:
              - effect: "NoSchedule"
                key: "node-role.kubernetes.io/${KATANA_CONTROL_VM_TYPE}"
                operator: "Equal"
                value: "true"
        #  This is new, from DeepCDR
        #
        worker:
          spec:
            containers:
              - name: dask-worker
   
                args:
                  - dask-worker
                  - --no-dashboard
                  - --name=$(DASK_WORKER_NAME)
                  - --nworkers=16
                  - --nthreads=1
                  #- --no-nanny
   
                image: |-
                  {{- include "dask.workerImageName" . -}}
                imagePullPolicy: "IfNotPresent"


  #  This is new, from DeepCDR
  #
  deployment:
    #
    #  These lines should align with 'resources'
    #  Including them throws this error,
    #
    #     >>>coalesce.go:220: warning: cannot overwrite table with non table for katana.katana.deployment.nodeSelector (map[])
    #
    #  nodeSelector: |
    #    node-role.kubernetes.io:${KATANA_CONTROL_VM_TYPE} 

    resources:
      limits:
        cpu: 500m
        memory: 256Mi
      requests:
        cpu: 500m
        memory: 256Mi
   
    tolerations: |
      - effect: "NoSchedule"
        key: "node-role.kubernetes.io/${KATANA_CONTROL_VM_TYPE}"
        operator: "Equal"
        value: "true"


#  ZZZZZZZZZZZZZZZZZZZZZZZZZ


  #  MPI administrative stuff-
  #
  #  graph-worker-launcher
  #
  mpi:
    spec:
      mpiReplicaSpecs:
        Launcher:
          template:
            spec:
              nodeSelector:
                node.kubernetes.io/instance-type: ${KATANA_CONTROL_VM_TYPE}
              containers:
                - name: launcher
                  image: |-
                    {{- include "mpi.launcherImageName" . -}}
                  imagePullPolicy: IfNotPresent
                  resources:
                    limits:
                      cpu: 500m
                      memory: 256Mi
                    requests:
                      cpu: 500m
                      memory: 256Mi
              tolerations:
                - effect: "NoSchedule"
                  key: "node-role.kubernetes.io/katana-control-plane"
                  operator: "Equal"
                  value: "true"
                   
        Worker:
          template:
            spec:
              containers:
                ##### Application Specific ####
                - name: dask-worker
                  env:
                  - name: "OMP_NUM_THREADS"
                    value: "16"
                  - name: "DGLBACKEND"
                    value: "pytorch"
                  image: |-
                    {{- include "mpi.workerImageName" . -}}
                  imagePullPolicy: IfNotPresent
                  resources:
                    limits:
                      cpu: 31
                      memory: 230G
                    requests:
                      cpu: 31
                      memory: 230G
                         





